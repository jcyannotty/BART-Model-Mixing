plot_mu_trace = function(mu_matrix, wt_num, x_input, tree_nums = NULL){
title = paste0('Trace plots of mu',wt_num,'at ',x_input)
par(mfrow = c(1,3), oma = c(0,0,3,0))
for(i in 1:ncol(mu_matrix)){
if(!is.null(tree_nums)){
xname = paste0("Tree", tree_nums[i])
}else{
xname = paste0("Tree", i)
}
plot(mu_matrix[,i], xlab = 'Iteration', ylab = 'mu', main = xname, type = 'l',
panel.first = {grid(col = 'lightgrey')})
}
mtext(title, cex = 1.25, outer = TRUE)
}
#Generate training data
get_data = function(n_train, n_test, s, sg, lg, minx, maxx, seed){
#Get test and train data
set.seed(seed)
x_train = seq(minx, maxx, length = n_train)
y_train = fg(x_train) + rnorm(n_train, 0, s)
#Set a grid of test points
x_test = seq(minx, maxx, length = n_test)
fg_test = fg(x_test)
#Define the Model set -- a matrix where each column is an the output evaluated at the train/test pts
g_exp = c(sg, lg) #Mix both small and large g
K = length(g_exp)
Ks = length(sg) #Number of small-g models
Kl = length(lg) #Number of large-g models
#Define matrices to store the function values
f_train = matrix(0, nrow = n_train, ncol = K)
f_test = matrix(0, nrow = n_test, ncol = K)
#Define discrepancy information
f_train_dmean = matrix(0, nrow = n_train, ncol = K)
f_train_dsd = matrix(1, nrow = n_train, ncol = K)
f_test_dmean = matrix(0, nrow = n_test, ncol = K)
f_test_dsd = matrix(1, nrow = n_test, ncol = K)
#Computation
for(i in 1:K){
#Get the small g expansion output
if(i <= length(sg)){
f_train[,i] = sapply(x_train, fsg, ns = g_exp[i])
f_test[,i] = sapply(x_test, fsg, ns = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dsg, ns = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dsg, ns = g_exp[i])
}else{
#Get the large g expansion output
f_train[,i] = sapply(x_train, flg, nl = g_exp[i])
f_test[,i] = sapply(x_test, flg, nl = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dlg, nl = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dlg, nl = g_exp[i])
}
}
out = list(x_train = x_train, x_test = x_test, y_train = y_train, f_train = f_train,
f_test = f_test, fg_test = fg_test ,f_train_dmean = f_train_dmean, f_train_dsd = f_train_dsd)
return(out)
}
bart_prior = function(y.train, c, m, K){
tau = (max(y.train)-min(y.train))/(2*c*sqrt(m))
tau_matrix = diag(tau,K)
beta_hat = rep(0,K)
out = list(beta = beta_hat, tau_matrix = tau_matrix)
return(out)
}
#Non-Stationary Prior 1: Motivated by G-Prior and Raftery et al.
ns_prior1 = function(dsd.train,c,m){
K = ncol(dsd.train)
sum_var = apply(dsd.train, 2, sum)
#Get g-prior like covariance
tau_matrix = diag(1/(sum_var*c*sqrt(m)),K)
#get beta_hat
beta_hat = rep(0,K)
out = list(beta = beta_hat, tau_matrix = tau_matrix)
return(out)
}
#Non-Stationary Prior 2: Beta is based on efficiency and tau is shrunk symmetrically
ns_prior2 = function(dsd.train,c,m){
K = ncol(dsd.train)
#Get total precision for each point
sum_precision = apply(1/dsd.train^2, 1, sum)
#Get estimate of the weights
w_hat = (1/dsd.train^2)/sum_precision
#Get beta hat
beta_hat = apply(w_hat,2,mean)
#Get tau for each function
tau_vec = 0
for(i in 1:K){
#tau_vec[i] = (1/(2*c*sqrt(m)))*(1/(1+2*abs(beta_hat[i]-0.5)))
if(beta_hat[i]>0.5){
tau_vec[i] = (1-beta_hat[i])/(c*sqrt(m))
#tau_vec[i] = (1/(2*c*sqrt(m)))*(1/(1+2*abs(beta_hat[i]-0.5)))
}else{
tau_vec[i] = (beta_hat[i])/(c*sqrt(m))
#tau_vec[i] = (1/(2*c*sqrt(m)))*(1/(1+2*abs(beta_hat[i]-0.5)))
}
}
#Now adjust beta by m
beta_hat = beta_hat/m
#Get into a matrix
tau_matrix = diag(tau_vec, K)
out = list(beta = beta_hat, tau_matrix = tau_matrix)
return(out)
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Plotting functions
plot_prior_marginal = function(beta_vec, tau_matrix, colnum, n.den = 500){
#Parameters
b = beta_vec[colnum]
tau = tau_matrix[colnum, colnum]
#Density Plot
lb = b - 4*tau
ub = b + 4*tau
x_grid = seq(lb, ub, length = n.den)
norm_den = dnorm(x_grid, mean = b, sd = tau)
plot(x_grid, norm_den, main = 'Prior', xlab = 'mu', ylab = 'Density',
panel.first = {grid(col = 'lightgrey')},
type = 'l', lwd = 2)
}
plot_prior_joint = function(beta_vec, tau_matrix, colnums = c(1,2), color = 'black',x_lim = NULL ,y_lim = NULL,
n.den = 500, title = 'Mu Prior', eta.id = 'p', tree.id = 'j'){
#Parameters
b = beta_vec[colnums]
tau = tau_matrix[colnums, colnums]
#Names
xname = paste0('Mu.',eta.id,tree.id,colnums[1])
yname = paste0('Mu.',eta.id,tree.id,colnums[2])
#Density Plot
x_grid = seq(-4, 4, length = n.den)
norm_grid = rnorm(n.den*2, mean = 0, sd = 1)
mu_matrix = matrix(0,nrow = n.den, ncol = 2)
for(i in 1:n.den){
mu_matrix[i,] = b + tau_matrix%*%norm_grid[(2*i-1):(2*i)]
}
if(is.null(x_lim) & is.null(y_lim)){
plot(mu_matrix[,1],mu_matrix[,2], main = title, xlab = xname, ylab = yname,
panel.first = {grid(col = 'lightgrey')}, pch = 16, cex = 1.5, col = color)
}else{
plot(mu_matrix[,1],mu_matrix[,2], main = title, xlab = xname, ylab = yname,col = color,
panel.first = {grid(col = 'lightgrey')}, pch = 16, cex = 1.5, xlim = x_lim, ylim = y_lim)
}
}
add_points = function(beta_vec, tau_matrix, colnums = c(1,2), n.den = 500){
#Parameters
b = beta_vec[colnums]
tau = tau_matrix[colnums, colnums]
#Density Plot
x_grid = seq(-4, 4, length = n.den)
norm_grid = rnorm(n.den*2, mean = 0, sd = 1)
mu_matrix = matrix(0,nrow = n.den, ncol = 2)
for(i in 1:n.den){
mu_matrix[i,] = b + tau_matrix%*%norm_grid[(2*i-1):(2*i)]
}
points(mu_matrix[,1],mu_matrix[,2], pch = 16, cex = 1.5, col = 'green')
}
add_curve = function(beta_vec, tau_matrix, colnum, n.den = 500){
#Parameters
b = beta_vec[colnum]
tau = tau_matrix[colnum, colnum]
#Density Plot
lb = b - 4*tau
ub = b + 4*tau
x_grid = seq(lb, ub, length = n.den)
norm_den = dnorm(x_grid, mean = b, sd = tau)
lines(x_grid, norm_den, lwd = 2, col='green')
}
ex_data = get_data(20,300,0.005,2,4,0.03,0.5,54321)
data_rng = list(c(1:4),c(5:12),c(13:20))
nsp2 = list()
bartp = bart_prior(ex_data$y_train, c = 2, m=10, K = 2)
tbl1 = c(bartp$beta, diag(bartp$tau_matrix))
par(mfrow = c(1,3))
for(i in 1:3){
nsp2[[i]] = ns_prior2(ex_data$f_train_dsd[data_rng[[i]],], c = 2, m=10)
plot_prior_joint(bartp$beta, bartp$tau_matrix, c(1,2),n.den = 1500, color = 'grey60',
x_lim = c(-0.175,0.175),y_lim = c(-0.175,0.175))
add_points(nsp2[[i]]$beta, nsp2[[i]]$tau_matrix,n.den = 1500)
abline(v = nsp2[[i]]$beta[1], col = 'green', lwd = 2, lty = 'dashed')
abline(h = nsp2[[i]]$beta[2], col = 'green', lwd = 2, lty = 'dashed')
tbl1 = rbind(tbl1, c(nsp2[[i]]$beta,diag(nsp2[[i]]$tau_matrix)))
}
tbl1 %>% kable() %>% kable_styling(full_width = FALSE)
library(kableExtra)
tbl1 %>% kable() %>% kable_styling(full_width = FALSE)
tbl1 %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>% kable() %>% kable_styling(full_width = FALSE)
as.data.frame(tbl1) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>% kable() %>% kable_styling(full_width = FALSE)
ex_data = get_data(20,300,0.005,2,4,0.03,0.5,54321)
data_rng = list(c(1:3),c(4:9),c(10:17),c(18,20))
nsp2 = list()
bartp = bart_prior(ex_data$y_train, c = 2, m=10, K = 2)
tbl2 = c(bartp$beta, diag(bartp$tau_matrix))
par(mfrow = c(2,2))
for(i in 1:4){
nsp2[[i]] = ns_prior2(ex_data$f_train_dsd[data_rng[[i]],], c = 2, m=10)
plot_prior_joint(bartp$beta, bartp$tau_matrix, c(1,2),n.den = 1500,color = 'grey60',
x_lim = c(-0.175,0.175),y_lim = c(-0.175,0.175))
add_points(nsp2[[i]]$beta, nsp2[[i]]$tau_matrix,n.den = 1500)
abline(v = nsp2[[i]]$beta[1], col = 'green', lwd = 2, lty = 'dashed')
abline(h = nsp2[[i]]$beta[2], col = 'green', lwd = 2, lty = 'dashed')
tbl2 = rbind(tbl2, c(nsp2[[i]]$beta,diag(nsp2[[i]]$tau_matrix)))
}
colnames(tbl2) = c('beta1', 'beta2', 'tau1', 'tau2')
rownames(tbl2) = c('Stationary BART', 'Non-Stationary: Node 1', 'Non-Stationary: Node 2',
'Non-Stationary: Node 3','Non-Stationary: Node 4')
as.data.frame(tbl2) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
kable() %>%
kable_styling(full_width = FALSE)
beta_grid = seq(0.0001, 0.9999, length = 200)
c = 2
m = 10
tau_grid = ifelse(beta_grid>0.5, (beta_grid-0.5)/(c*sqrt(m)), (beta_grid)/(c*sqrt(m)) )
plot(beta_grid, tau_grid)
beta_grid = seq(0.0001, 0.9999, length = 200)
c = 2
m = 10
tau_grid = ifelse(beta_grid>0.5, (1-beta_grid)/(c*sqrt(m)), (beta_grid)/(c*sqrt(m)) )
plot(beta_grid, tau_grid)
beta_grid = seq(0.0001, 0.9999, length = 200)
c = 2
m = 10
tau_grid = ifelse(beta_grid>0.5, (1-beta_grid)/(c*sqrt(m)), (beta_grid)/(c*sqrt(m)))
plot(beta_grid, tau_grid, type = 'l', lwd = 2, panel.first = {grid(col='lightgrey')}, main = 'Beta vs Tau',
xlab = 'beta', ylab = 'tau')
beta_grid = seq(0.0001, 0.9999, length = 200)
c = 2
m = 10
tau_grid = ifelse(beta_grid>0.5, (1-beta_grid)/(c*sqrt(m)), (beta_grid)/(c*sqrt(m)))
plot(beta_grid, tau_grid, type = 'l', lwd = 2, panel.first = {grid(col='lightgrey')}, main = 'Beta vs Tau (c = 2, m = 10)',
xlab = 'beta', ylab = 'tau')
title_list = paste('Mu Prior: Node', 1:4)
title_list
ex_data = get_data(20,300,0.005,2,4,0.03,0.5,54321)
data_rng = list(c(1:3),c(4:9),c(10:17),c(18,20))
nsp2 = list()
bartp = bart_prior(ex_data$y_train, c = 2, m=10, K = 2)
tbl2 = c(bartp$beta, diag(bartp$tau_matrix))
title_list = paste('Mu Prior: Node', 1:4)
par(mfrow = c(2,2))
for(i in 1:4){
nsp2[[i]] = ns_prior2(ex_data$f_train_dsd[data_rng[[i]],], c = 2, m=10)
plot_prior_joint(bartp$beta, bartp$tau_matrix, c(1,2),n.den = 1500,color = 'grey60',
x_lim = c(-0.175,0.175),y_lim = c(-0.175,0.175), title = title_list[i])
add_points(nsp2[[i]]$beta, nsp2[[i]]$tau_matrix,n.den = 1500)
abline(v = nsp2[[i]]$beta[1], col = 'green', lwd = 2, lty = 'dashed')
abline(h = nsp2[[i]]$beta[2], col = 'green', lwd = 2, lty = 'dashed')
tbl2 = rbind(tbl2, c(nsp2[[i]]$beta,diag(nsp2[[i]]$tau_matrix)))
}
colnames(tbl2) = c('beta1', 'beta2', 'tau1', 'tau2')
rownames(tbl2) = c('Stationary BART', 'Non-Stationary: Node 1', 'Non-Stationary: Node 2',
'Non-Stationary: Node 3','Non-Stationary: Node 4')
as.data.frame(tbl2) %>% mutate_if(is.numeric, funs(as.character(signif(., 3)))) %>%
kable() %>%
kable_styling(full_width = FALSE)
beta_grid = seq(0.0001, 0.9999, length = 200)
c = 2
m = 10
tau_grid = ifelse(beta_grid>0.5, (1-beta_grid)/(c*sqrt(m)), (beta_grid)/(c*sqrt(m)))
plot(beta_grid, tau_grid, type = 'l', lwd = 2, panel.first = {grid(col='lightgrey')}, main = 'Beta vs Tau (c = 2, m = 10)',
xlab = 'beta', ylab = 'tau')
fitp1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_1p.rds")
fitw1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_1w.rds")
fitp2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_2bp.rds")
fitw2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_2bw.rds")
ex1_data = get_data(20, 300, 0.005, 2, 4, 0.03, 0.5, 54321)
attach(exmpi_data)
attach(ex1_data)
par(mfrow = c(1,3), oma = c(0,0,2,0))
plot_fit(fitp_s2, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'Original BART')
attach(exmpi_data)
fitp_s1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn5p.rds")
fitw_s1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn5w.rds")
fitp_s2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn2p.rds")
fitw_s2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn2w.rds")
fitp_s3 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn1p.rds")
fitw_s3 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_SP_mn1w.rds")
exmpi_data = get_data(20, 300, 0.005, 2, 4, 0.03, 0.5, 54321)
fitp1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_1p.rds")
fitw1 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_1w.rds")
fitp2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_2bp.rds")
fitw2 = readRDS("Open BT Examples/Non-Stationary Priors 03-14/sg2lg4_2bw.rds")
ex1_data = get_data(20, 300, 0.005, 2, 4, 0.03, 0.5, 54321)
attach(ex1_data)
par(mfrow = c(1,3), oma = c(0,0,2,0))
plot_fit(fitp_s2, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'Original BART')
plot_fit(fitp1, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'Original Non-Stationary')
plot_fit(fitp2, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'New Non-Stationary')
mtext("Model Mixing Predicted Mean", outer = TRUE, cex = 1.25)
detach(ex1_data)
attach(ex1_data)
par(mfrow = c(1,3), oma = c(0,0,2,0))
plot_fit(fitp_s2, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'Original BART')
plot_fit(fitp1, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'Original Non-Stationary')
plot_fit(fitp2, x_train,y_train, x_test, fg_test, f_test, y_lim = c(1.5,2.8), title = 'New Non-Stationary')
mtext("Model Mixing Predicted Mean", outer = TRUE, cex = 1.25)
detach(ex1_data)
attach(ex1_data)
par(mfrow = c(1,3), oma = c(0,0,2,0))
plot_wts(fitw_s2, x_test,y_lim = c(-0.1,1.2), title = 'Original Stationary')
plot_wts(fitw1, x_test,y_lim = c(-0.1,1.2), title = 'Original Non-Stationary')
plot_wts(fitw2, x_test,y_lim = c(-0.1,1.2), title = 'New Non-Stationary')
mtext("Model Mixing Weights", outer = TRUE, cex = 1.25)
detach(ex1_data)
ex_data$x_train
#example_model_mixing.R
setwd("/home/johnyannotty/Documents/Open BT Project SRC")
# Load the R wrapper functions to the OpenBT library.
source("/home/johnyannotty/Documents/Open BT Project SRC/openbt.R")
fg = function(g){
x = 1/(32*g^2)
K = besselK(x = x, nu = 0.25)
b = exp(x)/(2*sqrt(2)*g)*K
return(b)
}
#Construct the small-g expansion
fsg = function(g, ns){
#Get the term number
k = 0:ns
#Get the coefficients -- only even coefficients are non-zero
sk = ifelse(k %% 2 == 0,sqrt(2)*gamma(k + 1/2)*(-4)^(k/2)/factorial(k/2), 0)
#Get the expansion
f = sum(sk*g^k)
return(f)
}
#Construct the large-g expansion
flg = function(g, nl){
#Get the term number
k = 0:nl
#Get the coefficients
lk = gamma(k/2 + 0.25)*(-0.5)^k/(2*factorial(k))
#Get the expansion
f = sum(lk*g^(-k))/sqrt(g)
return(f)
}
#Construct the small-g discrepancy
dsg = function(g,ns){
#Get the term number
k = 0:ns
#Get the coefficients -- only even coefficients are non-zero
sk = ifelse(k %% 2 == 0,sqrt(2)*gamma(k + 1/2)*(-4)^(k/2)/factorial(k/2), 0)
#Estimate cbar
cbar = sqrt(mean(sk^2))
#get the variance estimate
if((ns/2)%%2 == 0){
v = (cbar^2)*(factorial(ns/2 + 2)^2)*g^(ns + 4)
}else{
v = (cbar^2)*(factorial(ns/2 + 1)^2)*g^(ns + 2)
}
s = sqrt(v)
return(s)
}
#Construct the large-g discrepancy
dlg = function(g,nl){
#Get the term number
k = 0:nl
#Get the coefficients
lk = gamma(k/2 + 0.25)*(-0.5)^k/(2*factorial(k))
#Estimate cbar
if(nl < 2){
print("Warning, nl < 2, dbar is not estimated as intended")
dbar = 1
}else{
#Estimate cbar using coefs of order 2 through nl
dbar = sqrt(mean(lk[-c(1,2)]^2))
}
#Get standard deviation
v = (dbar^2)*(1/(factorial(nl+1)^2))*(1/g^(2*nl + 3))
s = sqrt(v)
return(s)
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Graph the expansions and the true function.
#Create a grid of g's
g_grid = seq(0.01, 0.5, length = 300)
n_train = 20
n_test = 300
s = 0.005 #s = 0.03
set.seed(54321)
x_train = seq(0.03, 0.5, length = n_train)
y_train = fg(x_train) + rnorm(n_train, 0, s)
#Set a grid of test points
x_test = seq(0.03, 0.5, length = n_test)
fg_test = fg(x_test)
fs_test = sapply(x_test, fsg, 2)
fl_test = sapply(x_test, flg, 4)
#Plot the training data
plot(x_train, y_train, pch = 16, cex = 0.8, main = 'Training data')
lines(g_grid, fg(g_grid))
small_g = 2 #Which small-g expansions to use
large_g = 4 #Which large-g expansions to use
g_exp = c(small_g, large_g) #Mix both small and large g
K = length(g_exp)
Ks = length(small_g) #Number of small-g models
Kl = length(large_g) #Number of large-g models
#Define matrices to store the function values
f_train = matrix(0, nrow = n_train, ncol = K)
f_test = matrix(0, nrow = n_test, ncol = K)
#Define discrepancy information
f_train_dmean = matrix(0, nrow = n_train, ncol = K)
f_train_dsd = matrix(1, nrow = n_train, ncol = K)
f_test_dmean = matrix(0, nrow = n_test, ncol = K)
f_test_dsd = matrix(1, nrow = n_test, ncol = K)
#Computation
for(i in 1:K){
#Get the small g expansion output
if(i <= length(small_g)){
f_train[,i] = sapply(x_train, fsg, ns = g_exp[i])
f_test[,i] = sapply(x_test, fsg, ns = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dsg, ns = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dsg, ns = g_exp[i])
}else{
#Get the large g expansion output
f_train[,i] = sapply(x_train, flg, nl = g_exp[i])
f_test[,i] = sapply(x_test, flg, nl = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dlg, nl = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dlg, nl = g_exp[i])
}
}
#Cast x_train and x_test as matrices
x_train = as.matrix(x_train, ncol = 1)
x_test = as.matrix(x_test, ncol = 1)
fit=openbt(x_train,y_train,f_train,pbd=c(0.7,0.0),ntree = 7,ntreeh=1,numcut=300,tc=4,model="mixbart",modelname="physics_model",
ndpost = 10000, nskip = 2000, nadapt = 5000, adaptevery = 500, printevery = 500,
power = 1.0, minnumbot = 2, overallsd = sd(y_train)/sqrt(100), k = 2,
f.discrep.mean =  f_train_dmean, f.discrep.sd = f_train_dsd)
fitp=predict.openbt(fit,x.test = x_test, f.test = f_test, f.discrep.mean =  f_test_dmean, f.discrep.sd = f_test_dsd ,tc=4)
plot(g_grid, fg(g_grid), pch = 16, cex = 0.8, main = 'Fits', type = 'l', ylim = c(1.8,2.8))
points(x_train, y_train, pch = 3)
lines(x_test, f_test[,1], col = 'red', lty = 2)
lines(x_test, f_test[,2], col = 'blue', lty = 2)
points(x_test, fitp$mmean, col = 'green4')
points(x_test, fitp$m.lower, col = 'orange')
points(x_test, fitp$m.upper, col = 'orange')
plot(g_grid, fg(g_grid), pch = 16, cex = 0.8, main = 'Fits', type = 'l', ylim = c(1.8,2.8))
points(x_train, y_train, pch = 3)
lines(x_test, f_test[,1], col = 'red', lty = 2)
lines(x_test, f_test[,2], col = 'blue', lty = 2)
lines(x_test, fitp$mmean, col = 'green4')
lines(x_test, fitp$m.lower, col = 'orange')
lines(x_test, fitp$m.upper, col = 'orange')
small_g = 4 #Which small-g expansions to use
large_g = 4 #Which large-g expansions to use
g_exp = c(small_g, large_g) #Mix both small and large g
K = length(g_exp)
Ks = length(small_g) #Number of small-g models
Kl = length(large_g) #Number of large-g models
#Define matrices to store the function values
f_train = matrix(0, nrow = n_train, ncol = K)
f_test = matrix(0, nrow = n_test, ncol = K)
#Define discrepancy information
f_train_dmean = matrix(0, nrow = n_train, ncol = K)
f_train_dsd = matrix(1, nrow = n_train, ncol = K)
f_test_dmean = matrix(0, nrow = n_test, ncol = K)
f_test_dsd = matrix(1, nrow = n_test, ncol = K)
#Computation
for(i in 1:K){
#Get the small g expansion output
if(i <= length(small_g)){
f_train[,i] = sapply(x_train, fsg, ns = g_exp[i])
f_test[,i] = sapply(x_test, fsg, ns = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dsg, ns = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dsg, ns = g_exp[i])
}else{
#Get the large g expansion output
f_train[,i] = sapply(x_train, flg, nl = g_exp[i])
f_test[,i] = sapply(x_test, flg, nl = g_exp[i])
f_train_dsd[,i] = sapply(x_train, dlg, nl = g_exp[i])
f_test_dsd[,i] = sapply(x_test, dlg, nl = g_exp[i])
}
}
#Cast x_train and x_test as matrices
x_train = as.matrix(x_train, ncol = 1)
x_test = as.matrix(x_test, ncol = 1)
fit=openbt(x_train,y_train,f_train,pbd=c(0.7,0.0),ntree = 10,ntreeh=1,numcut=300,tc=4,model="mixbart",modelname="physics_model",
ndpost = 10000, nskip = 2000, nadapt = 5000, adaptevery = 500, printevery = 500,
power = 1.0, minnumbot = 2, overallsd = sd(y_train)/sqrt(100), k = 2,
f.discrep.mean =  f_train_dmean, f.discrep.sd = f_train_dsd)
fitp=predict.openbt(fit,x.test = x_test, f.test = f_test, f.discrep.mean =  f_test_dmean, f.discrep.sd = f_test_dsd ,tc=4)
plot(g_grid, fg(g_grid), pch = 16, cex = 0.8, main = 'Fits', type = 'l', ylim = c(1.8,2.8))
points(x_train, y_train, pch = 3)
lines(x_test, f_test[,1], col = 'red', lty = 2)
lines(x_test, f_test[,2], col = 'blue', lty = 2)
points(x_test, fitp$mmean, col = 'green4')
points(x_test, fitp$m.lower, col = 'orange')
points(x_test, fitp$m.upper, col = 'orange')
fitw = openbt.mixingwts(fit, x.test = x_test, numwts = 2, tc = 4)
#Plot model weights
plot(x_test, fitw$wmean[,1], pch = 16, col = 'red', type = 'o', ylim = c(-1,2))
points(x_test, fitw$wmean[,2], col = 'blue', pch = 16)
lines(x_test, fitw$w.upper[,1], col = 'red', lty = 'dashed')
lines(x_test, fitw$w.lower[,1], col = 'red', lty = 'dashed')
lines(x_test, fitw$w.upper[,2], col = 'blue', lty = 'dashed')
lines(x_test, fitw$w.lower[,2], col = 'blue', lty = 'dashed')
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Model Mixing with OpenBT
fit=openbt(x_train,y_train,f_train,pbd=c(0.7,0.0),ntree = 25,ntreeh=1,numcut=300,tc=4,model="mixbart",modelname="physics_model",
ndpost = 10000, nskip = 1000, nadapt = 5000, adaptevery = 500, printevery = 500,
power = 1.0, minnumbot = 3, overallsd = sd(y_train)/sqrt(100), k = 2)
fitp=predict.openbt(fit,x.test = x_test, f.test = f_test,tc=4)
plot(g_grid, fg(g_grid), pch = 16, cex = 0.8, main = 'Fits', type = 'l', ylim = c(1.8,2.8))
points(x_train, y_train, pch = 3)
lines(x_test, f_test[,1], col = 'red', lty = 2)
lines(x_test, f_test[,2], col = 'blue', lty = 2)
points(x_test, fitp$mmean, col = 'green4')
points(x_test, fitp$m.lower, col = 'orange')
points(x_test, fitp$m.upper, col = 'orange')
fitw = openbt.mixingwts(fit, x.test = x_test, numwts = 2, tc = 4)
#Plot model weights
plot(x_test, fitw$wmean[,1], pch = 16, col = 'red', type = 'o', ylim = c(-1,1))
points(x_test, fitw$wmean[,2], col = 'blue', pch = 16)
lines(x_test, fitw$w.upper[,1], col = 'red', lty = 'dashed')
lines(x_test, fitw$w.lower[,1], col = 'red', lty = 'dashed')
lines(x_test, fitw$w.upper[,2], col = 'blue', lty = 'dashed')
lines(x_test, fitw$w.lower[,2], col = 'blue', lty = 'dashed')
